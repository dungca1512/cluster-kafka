# mongodb_exporter.py
import pymongo
import json
import sys
import time
from datetime import datetime
from loguru import logger

# C·∫•u h√¨nh loguru v·ªõi m√†u s·∫Øc
logger.remove()  # X√≥a c·∫•u h√¨nh m·∫∑c ƒë·ªãnh
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    colorize=True,  # ƒê·∫£m b·∫£o b·∫≠t m√†u
    level="INFO"
)
logger.add(
    "mongodb_export.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    rotation="10 MB",
    level="INFO"
)

def connect_to_mongodb(uri, db_name):
    """K·∫øt n·ªëi ƒë·∫øn MongoDB"""
    try:
        client = pymongo.MongoClient(uri)
        db = client[db_name]
        # Ki·ªÉm tra k·∫øt n·ªëi
        client.admin.command('ping')
        logger.info(f"ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn MongoDB: {db_name}")
        return client, db
    except Exception as e:
        logger.error(f"L·ªói k·∫øt n·ªëi ƒë·∫øn MongoDB: {str(e)}")
        sys.exit(1)

def export_batch(collection, query, batch_size, skip, output_file, append=False):
    """Xu·∫•t m·ªôt batch d·ªØ li·ªáu t·ª´ MongoDB v√† l∆∞u v√†o file"""
    try:
        start_time = time.time()
        # Truy v·∫•n d·ªØ li·ªáu
        cursor = collection.find(query).skip(skip).limit(batch_size)
        
        # Ch·∫ø ƒë·ªô ghi file (ghi m·ªõi ho·∫∑c append)
        mode = 'a' if append else 'w'
        count = 0
        
        with open(output_file, mode, encoding='utf-8') as f:
            # N·∫øu l√† file m·ªõi, m·ªü array JSON
            if not append:
                f.write('[\n')
            else:
                # N·∫øu append, th√™m d·∫•u ph·∫©y sau ph·∫ßn t·ª≠ cu·ªëi c√πng
                f.seek(0, 2)  # Di chuy·ªÉn con tr·ªè ƒë·∫øn cu·ªëi file
                pos = f.tell()  # L·∫•y v·ªã tr√≠ hi·ªán t·∫°i
                if pos > 2:  # N·∫øu file kh√¥ng r·ªóng v√† c√≥ d·ªØ li·ªáu
                    f.seek(pos - 2)  # L√πi l·∫°i 2 k√Ω t·ª± (\n])
                    f.write(',\n')  # Thay th·∫ø \n] b·∫±ng ,\n
                else:
                    f.write('[\n')  # N·∫øu file r·ªóng, b·∫Øt ƒë·∫ßu array
            
            # Ghi t·ª´ng document v√†o file
            for doc in cursor:
                count += 1
                # Chuy·ªÉn ObjectId th√†nh string
                doc['_id'] = str(doc['_id'])
                # Chuy·ªÉn date th√†nh string ISO format
                for key, value in doc.items():
                    if isinstance(value, datetime):
                        doc[key] = value.isoformat()
                
                # Ghi document v√†o file
                json_str = json.dumps(doc, ensure_ascii=False)
                if count < batch_size:
                    f.write(json_str + ',\n')
                else:
                    f.write(json_str + '\n')
            
            # N·∫øu l√† batch cu·ªëi, ƒë√≥ng array JSON
            if count < batch_size:
                f.write(']')
            
        elapsed_time = time.time() - start_time
        logger.info(f"ƒê√£ xu·∫•t {count:,} b·∫£n ghi trong {elapsed_time:.2f} gi√¢y ({count/elapsed_time:.2f} b·∫£n ghi/gi√¢y)")
        
        return count
    
    except Exception as e:
        logger.error(f"L·ªói khi xu·∫•t d·ªØ li·ªáu: {str(e)}")
        return 0

def export_collection(uri, db_name, collection_name, output_file, query=None, batch_size=10000):
    """Xu·∫•t to√†n b·ªô collection theo t·ª´ng batch"""
    if query is None:
        query = {}
    
    client, db = connect_to_mongodb(uri, db_name)
    collection = db[collection_name]
    
    try:
        # ƒê·∫øm t·ªïng s·ªë b·∫£n ghi
        total_records = collection.count_documents(query)
        logger.info(f"T·ªïng s·ªë b·∫£n ghi: {total_records:,}")
        
        # Xu·∫•t d·ªØ li·ªáu theo t·ª´ng batch
        processed = 0
        start_time = time.time()
        
        while processed < total_records:
            batch_count = export_batch(collection, query, batch_size, processed, output_file, append=(processed > 0))
            processed += batch_count
            
            # Hi·ªÉn th·ªã ti·∫øn ƒë·ªô
            progress = (processed / total_records) * 100
            logger.info(f"Ti·∫øn ƒë·ªô: {processed:,}/{total_records:,} ({progress:.2f}%)")
            
            # N·∫øu kh√¥ng c√≤n d·ªØ li·ªáu n√†o ƒë∆∞·ª£c xu·∫•t, tho√°t kh·ªèi v√≤ng l·∫∑p
            if batch_count == 0:
                break
        
        total_time = time.time() - start_time
        avg_speed = processed/total_time if total_time > 0 else 0
        logger.success(f"‚úÖ Ho√†n th√†nh xu·∫•t {processed:,} b·∫£n ghi trong {total_time:.2f} gi√¢y ({avg_speed:.2f} b·∫£n ghi/gi√¢y)")
    
    except Exception as e:
        logger.exception(f"L·ªói: {str(e)}")
    finally:
        client.close()
        logger.info("ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB")

if __name__ == "__main__":
    # In th√¥ng tin banner v·ªõi m√†u s·∫Øc
    logger.info("üöÄ === MongoDB Collection Exporter ===")
    
    # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
    uri = "mongodb://localhost:27017"
    db_name = "user_fcm"
    collection_name = "users_fcm"
    output_file = "output.json"
    batch_size = 10000
    
    # ƒê·ªçc tham s·ªë t·ª´ command line
    if len(sys.argv) > 1:
        uri = sys.argv[1]
    if len(sys.argv) > 2:
        db_name = sys.argv[2]
    if len(sys.argv) > 3:
        collection_name = sys.argv[3]
    if len(sys.argv) > 4:
        output_file = sys.argv[4]
    if len(sys.argv) > 5:
        try:
            batch_size = int(sys.argv[5])
        except ValueError:
            logger.warning(f"‚ö†Ô∏è K√≠ch th∆∞·ªõc batch kh√¥ng h·ª£p l·ªá: {sys.argv[5]}, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh: 10000")
    
    # In th√¥ng tin c·∫•u h√¨nh
    logger.info("üìã C·∫•u h√¨nh:")
    logger.info(f"  ‚Ä¢ URI: {uri}")
    logger.info(f"  ‚Ä¢ Database: {db_name}")
    logger.info(f"  ‚Ä¢ Collection: {collection_name}")
    logger.info(f"  ‚Ä¢ Output file: {output_file}")
    logger.info(f"  ‚Ä¢ Batch size: {batch_size:,}")
    
    # Query (t√πy ch·ªçn)
    query = {}  # Truy v·∫•n t·∫•t c·∫£ document
    # V√≠ d·ª•: query = {"status": "active"} # Ch·ªâ l·∫•y document c√≥ status=active
    
    # Ch·∫°y export
    logger.info(f"üîÑ B·∫Øt ƒë·∫ßu xu·∫•t d·ªØ li·ªáu t·ª´ {db_name}.{collection_name} sang {output_file}")
    export_collection(uri, db_name, collection_name, output_file, query, batch_size)